{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64a503d9-9233-450e-9f57-a15356bc4654",
   "metadata": {},
   "source": [
    "# ML lifecycle automation tools\n",
    "\n",
    "## MLflow\n",
    "\n",
    "MLflow is an open-source platform developed by Databricks that allows ML practitioners, such as data scientists and ML engineerings, to automate the complete end-to-end machine learning lifecycle using a simple and intuitive code interface. It includes tools for tracking experiments, packaging code into reproducible runs and sharing and deploying models. Its open interface design can work with any language or platform, with clients in Python and Java, and is accessible through a REST API. \n",
    "\n",
    "MLflow provides APIs for logging parameters, code versions, metrics, and artifacts when running your machine learning code and for later visualizing the results. It also provides a library of reusable components for machine learning tasks, such as pre-processing data and training models, that can be packaged into a reusable, reproducible run.\n",
    "\n",
    "In addition, MLflow integrates with a variety of machine learning libraries, including TensorFlow, PyTorch, and sci-kit-learn, allowing you to use the tools you are already familiar with while taking advantage of the advanced tracking and management capabilities of MLflow.\n",
    "\n",
    "For this post, you will need the following prerequisites:\n",
    "\n",
    "- The latest version of Docker installed in your machine. In case you don't have the latest version, please follow the instructions at the following URL: https://docs.docker.com/get-docker/.\n",
    "- Access to a bash terminal (Linux or Windows).\n",
    "- Access to a browser.\n",
    "- Python 3.5+ installed.\n",
    "- PIP installed.\n",
    "\n",
    "## Why MLflow?\n",
    "\n",
    "MLflow provides a single platform for everyday practitioners to handle the entire machine learning lifecycle, from iterating on model development to deploying it in a scalable and reliable environment that meets modern software system requirements.It facilitates the colaboration across roles and standarize the set of tools use a high level.\n",
    "\n",
    "### Getting started with Mlflow\n",
    "\n",
    "In the following code example, the Tracking API will be introduced. It would help us better understand how the MLflow API works and how it can be used to track our experiment's metrics, parameters, and artifacts during experimentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f32bb2d1-56af-47a0-97e9-9ba91b1eb4c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "from mlflow import log_metric, log_param, log_artifact\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.entities import ViewType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78d521d6-fb31-4262-a7e4-b439bd0ae1c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"http://0.0.0.0:4000\")\n",
    "# Log a parameter (key-value pair)\n",
    "log_param(\"param1\", 5)\n",
    "log_param(\"param3\", 2023)\n",
    "log_param(\"param4\", 2024)\n",
    "\n",
    "# Log a metric; metrics can be updated throughout the run\n",
    "log_metric(\"foo\", 1)\n",
    "log_metric(\"foo\", 2)\n",
    "log_metric(\"mse\", 0.5)\n",
    "\n",
    "# Log an artifact (output file)\n",
    "with open(\"output.txt\", \"w\") as f:\n",
    "    f.write(\"Hello world from mlflow!\")\n",
    "log_artifact(\"output.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5572d992-285c-4289-92f7-6e974299228c",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can observe that a new folder named `mlruns` was created along with some files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e365769-26c3-41a6-b184-479a0a0b09f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DISTRIB_ID=Ubuntu\n",
      "DISTRIB_RELEASE=22.04\n",
      "DISTRIB_CODENAME=jammy\n",
      "DISTRIB_DESCRIPTION=\"Ubuntu 22.04.1 LTS\"\n",
      "PRETTY_NAME=\"Ubuntu 22.04.1 LTS\"\n",
      "NAME=\"Ubuntu\"\n",
      "VERSION_ID=\"22.04\"\n",
      "VERSION=\"22.04.1 LTS (Jammy Jellyfish)\"\n",
      "VERSION_CODENAME=jammy\n",
      "ID=ubuntu\n",
      "ID_LIKE=debian\n",
      "HOME_URL=\"https://www.ubuntu.com/\"\n",
      "SUPPORT_URL=\"https://help.ubuntu.com/\"\n",
      "BUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\n",
      "PRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\n",
      "UBUNTU_CODENAME=jammy\n"
     ]
    }
   ],
   "source": [
    "!cat /etc/*-release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c7b714f-aa99-42dc-803b-13493201b0c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mmlruns\u001b[0m\n",
      "├── \u001b[01;34m0\u001b[0m\n",
      "│   ├── \u001b[01;34m5359bd93287c4650a405b4e1e2e1d927\u001b[0m\n",
      "│   │   ├── \u001b[01;34martifacts\u001b[0m\n",
      "│   │   ├── \u001b[00mmeta.yaml\u001b[0m\n",
      "│   │   ├── \u001b[01;34mmetrics\u001b[0m\n",
      "│   │   │   ├── \u001b[00mfoo\u001b[0m\n",
      "│   │   │   └── \u001b[00mmse\u001b[0m\n",
      "│   │   ├── \u001b[01;34mparams\u001b[0m\n",
      "│   │   │   ├── \u001b[00mparam1\u001b[0m\n",
      "│   │   │   └── \u001b[00mparam3\u001b[0m\n",
      "│   │   └── \u001b[01;34mtags\u001b[0m\n",
      "│   │       ├── \u001b[00mmlflow.runName\u001b[0m\n",
      "│   │       ├── \u001b[00mmlflow.source.name\u001b[0m\n",
      "│   │       ├── \u001b[00mmlflow.source.type\u001b[0m\n",
      "│   │       └── \u001b[00mmlflow.user\u001b[0m\n",
      "│   └── \u001b[00mmeta.yaml\u001b[0m\n",
      "└── \u001b[01;34mmodels\u001b[0m\n",
      "\n",
      "7 directories, 10 files\n"
     ]
    }
   ],
   "source": [
    "!tree mlruns -L 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db461fa7-40cc-4025-aba6-5535cb3f78fe",
   "metadata": {},
   "source": [
    "The mlruns folder is the default root directory for storing experiment runs and artifacts in the MLflow tracking component. The folder structure of the mlruns directory consists of multiple subdirectories, one for each run in your experiment.\n",
    "\n",
    "Each run subdirectory is named after a unique run ID and contains the following files and directories:\n",
    "\n",
    "- **meta.yaml:** A YAML file that contains metadata about the run, such as the run's status, start time, end time, and user-defined tags and parameters.\n",
    "- **params:** A directory that contains YAML files for each parameter used in the run. Each file is named after the parameter name and contains its value.\n",
    "- **tags:** A directory that contains YAML files for each tag applied to the run. Each file is named after the tag name and contains its value.\n",
    "- **artifacts:** A directory that contains files or directories generated as artifacts during the run. The contents of the artifacts directory are determined by the user and may include models, data files, plots, or other outputs.\n",
    "\n",
    "This structure allows for easy navigation of experiment runs, as well as efficient storage and retrieval of the metadata, parameters, tags, and artifacts associated with each run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c25e98b-4c84-46f1-9138-1a2d73e855ba",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Viewing the Tracking UI\n",
    "\n",
    "By default, wherever you run your program, the tracking API writes data into files into an mlruns directory. You can then run MLflow’s Tracking UI to visualize your experiments metadata and information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d68bf0f-c291-4958-a3e1-6a348af8314e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!mlflow server -h 0.0.0.0 -p 4000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc4e66a-5152-4910-990a-d69008f364b4",
   "metadata": {},
   "source": [
    "The experiments' metadata, runs information, parameters, and metrics can also be accessed programmatically by using the MLFlow client interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070e2e7a-8ee7-4df9-8368-a052ea92d81b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = MlflowClient()\n",
    "\n",
    "# list of experiments\n",
    "experiments_list = client.search_experiments() # returns a list of mlflow.entities.Experiment\n",
    "for experiment in experiments_list:\n",
    "    print(experiment.name, experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7daa4ad1-ee58-4483-8b43-2c32ee7de49e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Run: data=<RunData: metrics={'foo': 2.0, 'mse': 0.5}, params={'param1': '5', 'param3': '2023'}, tags={'mlflow.runName': 'rumbling-goose-465',\n",
      " 'mlflow.source.name': '/opt/conda/lib/python3.10/site-packages/ipykernel_launcher.py',\n",
      " 'mlflow.source.type': 'LOCAL',\n",
      " 'mlflow.user': 'jovyan'}>, info=<RunInfo: artifact_uri='mlflow-artifacts:/0/5359bd93287c4650a405b4e1e2e1d927/artifacts', end_time=None, experiment_id='0', lifecycle_stage='active', run_id='5359bd93287c4650a405b4e1e2e1d927', run_name='rumbling-goose-465', run_uuid='5359bd93287c4650a405b4e1e2e1d927', start_time=1676747061660, status='RUNNING', user_id='jovyan'>>\n"
     ]
    }
   ],
   "source": [
    "# list of runs\n",
    "runs_list = client.search_runs(\"0\") # returns a list of mlflow.entities.Experiment\n",
    "for run in runs_list:\n",
    "    print(run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e45e6e-3982-4be5-be5c-6e5e7817e82b",
   "metadata": {},
   "source": [
    "Let's review another example where we explore further the mlflow API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab278858-2521-471d-874b-0f7748a464bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: bd3ed7421ee64721b2ede3377bf508e7\n",
      "lifecycle_stage: active\n",
      "metrics: {'m': 2.5}\n",
      "tags: {tags}\n",
      "run_id: 6ad128918c884ce6b05ec005574375b4\n",
      "lifecycle_stage: active\n",
      "metrics: {'m': 1.55}\n",
      "tags: {tags}\n"
     ]
    }
   ],
   "source": [
    "# create experiment\n",
    "experiment_name = \"Social NLP Experiments\"\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "\n",
    "# create the experiment\n",
    "if experiment:\n",
    "    experiment_id = experiment.experiment_id\n",
    "else:\n",
    "    experiment_id = mlflow.create_experiment(\"Social NLP Experiments\")\n",
    "    \n",
    "# ends the currently active run, if any, taking an optional run status.\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8146ba96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# start a run\n",
    "# the with block defines the experiment scope\n",
    "with mlflow.start_run(experiment_id=experiment_id, run_name=\"run1\") as run1:\n",
    "    mlflow.log_metric(\"m\", 1.55)\n",
    "    mlflow.set_tag(\"s.release\", \"1.1.0-RC\")\n",
    "    mlflow.log_artifact(\"output.txt\")\n",
    "    \n",
    "    \n",
    "# start another run\n",
    "with mlflow.start_run(experiment_id=experiment_id, run_name=\"run2\") as run2:\n",
    "    mlflow.log_metric(\"m\", 2.50)\n",
    "    mlflow.set_tag(\"s.release\", \"1.2.0-GA\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64873ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search all runs under experiment id and order them by\n",
    "# descending value of the metric 'm'\n",
    "client = MlflowClient()\n",
    "runs = client.search_runs(experiment_id, order_by=[\"metrics.m DESC\"])\n",
    "for r in runs:\n",
    "    print(f\"run_id: {r.info.run_id}\")\n",
    "    print(f\"lifecycle_stage: {r.info.lifecycle_stage}\")\n",
    "    print(f\"metrics: {r.data.metrics}\")\n",
    "    # Exclude mlflow system tags\n",
    "    tags = {k: v for k, v in r.data.tags.items() if not k.startswith(\"mlflow.\")}\n",
    "    print(r\"tags: {tags}\")\n",
    "\n",
    "\n",
    "# Delete the first run\n",
    "client.delete_run(run_id=run1.info.run_id)\n",
    "\n",
    "# # Search only deleted runs under the experiment id and use a case insensitive pattern\n",
    "# in the filter_string for the tag.\n",
    "filter_string = \"tags.s.release ILIKE '%rc%'\"\n",
    "runs = client.search_runs(experiment_id, run_view_type=ViewType.DELETED_ONLY,\n",
    "                            filter_string=filter_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "638a406b-05e0-4efa-854c-6d90a9a711b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_best_run(experiment_id, metric):\n",
    "    # Connect to mlflow using the default connection\n",
    "    client = MlflowClient()\n",
    "    \n",
    "    # Get all the runs for the experiment\n",
    "    runs = client.search_runs(experiment_id)\n",
    "    \n",
    "    # Find the run with the highest accuracy metric\n",
    "    best_run = None\n",
    "    best_metric_value = 0\n",
    "    for run in runs:\n",
    "        metric_value = run.data.metrics[metric]\n",
    "        if metric_value > best_metric_value:\n",
    "            best_metric_value = metric_value\n",
    "            best_run = run\n",
    "    # Return the best run\n",
    "    return best_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "27857c56-b526-4e51-a30c-824817195ddb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Run: data=<RunData: metrics={'m': 2.5}, params={}, tags={'mlflow.runName': 'run2',\n",
       " 'mlflow.source.name': '/opt/conda/lib/python3.10/site-packages/ipykernel_launcher.py',\n",
       " 'mlflow.source.type': 'LOCAL',\n",
       " 'mlflow.user': 'jovyan',\n",
       " 's.release': '1.2.0-GA'}>, info=<RunInfo: artifact_uri='file:///home/jovyan/mlruns/850128866986301997/0e90a2c847b940b6b48388a365dd96fb/artifacts', end_time=1676745787687, experiment_id='850128866986301997', lifecycle_stage='active', run_id='0e90a2c847b940b6b48388a365dd96fb', run_name='run2', run_uuid='0e90a2c847b940b6b48388a365dd96fb', start_time=1676745787652, status='FINISHED', user_id='jovyan'>>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_best_run(experiment_id, metric=\"m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16857ed1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run():\n",
    "    for epoch in range(0, 3):\n",
    "        mlflow.log_metric(key=\"quality\", value=2*epoch, step=epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03793839-026a-415f-baf4-083ac9067b8f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## The Machine Learning Workflow\n",
    "\n",
    "Machine learning requires experimenting with a wide range of datasets, data preparation steps, and algorithms to build a model that maximizes some target metric. Once you have built a model, you also need to deploy it to a production system, monitor its performance, and continuously retrain it on new data and compare with alternative models.\n",
    "\n",
    "Being productive with machine learning can therefore be challenging for several reasons:\n",
    "\n",
    "- **It’s difficult to keep track of experiments.** When you are just working with files on your laptop, or with an interactive notebook, how do you tell which data, code and parameters went into getting a particular result?\n",
    "- **It’s difficult to reproduce code.** Even if you have meticulously tracked the code versions and parameters, you need to capture the whole environment (for example, library dependencies) to get the same result again. This is especially challenging if you want another data scientist to use your code, or if you want to run the same code at scale on another platform (for example, in the cloud).\n",
    "- **There’s no standard way to package and deploy models.** Every data science team comes up with its own approach for each ML library that it uses, and the link between a model and the code and parameters that produced it is often lost.\n",
    "\n",
    "Moreover, although individual ML libraries provide solutions to some of these problems (for example, model serving), to get the best result you usually want to try multiple ML libraries. MLflow lets you train, reuse, and deploy models with any library and package them into reproducible steps that other data scientists can use as a “black box,” without even having to know which library you are using.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89e6b71c-8bad-44df-8987-5246d99816ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
